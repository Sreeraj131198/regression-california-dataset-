# California Housing Price Prediction üè†

A comprehensive regression analysis project using the California Housing dataset to predict median house prices using multiple machine learning algorithms.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0%2B-orange)
![Pandas](https://img.shields.io/badge/Pandas-Latest-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

---

## üìã Table of Contents
- [Dataset Description](#dataset-description)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Key Components](#key-components)
- [Results Summary](#results-summary)
- [Usage](#usage)
- [Technologies Used](#technologies-used)
- [Contributing](#contributing)
- [License](#license)

---

## üéØ Project Overview

This project implements and compares **5 different regression algorithms** to predict median house prices in California districts. The analysis includes comprehensive data preprocessing, model evaluation, hyperparameter tuning, and cross-validation to identify the optimal predictive model.

### Objectives
- Perform exploratory data analysis and preprocessing
- Implement multiple regression algorithms
- Compare model performance using standard metrics
- Optimize models through hyperparameter tuning
- Select the best-performing model with proper justification

---

## üìä Dataset Description

**Source:** California Housing Dataset (sklearn.datasets)

**Features:**
- `MedInc`: Median income in block group
- `HouseAge`: Median house age in block group
- `AveRooms`: Average number of rooms per household
- `AveBedrms`: Average number of bedrooms per household
- `Population`: Block group population
- `AveOccup`: Average number of household members
- `Latitude`: Block group latitude
- `Longitude`: Block group longitude

**Target Variable:**
- `MedHouseVal`: Median house value (in $100,000s)

**Dataset Size:** 20,640 samples √ó 8 features

---



---

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup Instructions

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/california-housing-prediction.git
cd california-housing-prediction
```

2. **Create a virtual environment** (recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Required Libraries
```
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
jupyter>=1.0.0
```

---

## üîë Key Components

### 1Ô∏è‚É£ Data Loading and Preprocessing

#### Loading the Dataset
```python
from sklearn.datasets import fetch_california_housing
import pandas as pd

housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df['MedHouseVal'] = housing.target
```

#### Preprocessing Steps
- ‚úÖ **Missing Value Check:** No missing values detected in the dataset
- ‚úÖ **Feature Scaling:** StandardScaler applied to normalize feature ranges
- ‚úÖ **Exploratory Data Analysis:**
  - Distribution analysis of all features
  - Correlation heatmap to identify feature relationships
  - Outlier detection using box plots
  - Target variable distribution analysis

**Scaling Choice Justification:**
- Used **StandardScaler** (z-score normalization) because:
  - Features have different scales (e.g., Latitude vs. Population)
  - Required for SVR and regularized models
  - Improves convergence in gradient-based algorithms

---

### 2Ô∏è‚É£ Regression Algorithm Implementation

#### Algorithms Implemented

| Algorithm | Description | Suitability for Dataset |
|-----------|-------------|------------------------|
| **Linear Regression** | Simple linear relationship modeling | Baseline model; assumes linear relationships |
| **Decision Tree** | Non-parametric tree-based splits | Captures non-linear patterns; handles feature interactions |
| **Random Forest** | Ensemble of decision trees | Robust to outliers; reduces overfitting; handles non-linearity |
| **Gradient Boosting** | Sequential boosting of weak learners | High accuracy; learns complex patterns iteratively |
| **Support Vector Regressor** | Kernel-based regression | Effective in high-dimensional spaces with proper tuning |

#### Implementation Example
```python
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'SVR': SVR()
}
```

---

### 3Ô∏è‚É£ Model Evaluation and Comparison

#### Evaluation Metrics

**Before Hyperparameter Tuning:**


|model |              MSE|      MAE|      R2|     RMSE|
|Linear regression|  0.642187|0.579214| 0.509934|0.801366|
|Decision Tree |     0.787528|  |0.626855|  0.399021|0.88742|
|Random Forest|      0.422664|  |0.461930|  0.677456|  0.650126|
|Gradient Boost|     0.430989  |0.472077  |0.671103 | 0.656498|
|SVR                |0.594203  |0.532696  |0.546552  |0.770845|

**After Hyperparameter Tuning:**

 |model|               | MSE |   | RMSE|    |MAE|    | R2|
|--------------|       |-------|-----------|---------|-----|
|Linear Regression    | 0.642187|  0.801366|  |0.579214|  0.509934|
|Decision Tree          | 0.525987|  0.725250|  0.509060|  0.598608|
|Random Forest          |0.417083|  0.645819|  0.457790|  0.681716|
|Gradient Boosting     | 0.416057  |0.645025  |0.458290  |0.682498|
|SVR                    |0.553213|  0.743783|  0.502685|  0.57783

#### Best Performing Model
**üèÜGradient Boosting**

Performance Metrics:
  ‚Ä¢ R¬≤ Score: 0.6825
  ‚Ä¢ RMSE: 0.6450
  ‚Ä¢ MAE: 0.4583



#### Worst Performing Model
**‚ùå Linear Regression**
- **R¬≤ Score:** 0.50 
- **Reason:** Dataset has non-linear relationships (e.g., geographic clustering, income thresholds) that linear models cannot capture

---

### 4Ô∏è‚É£ Cross-Validation and Hyperparameter Tuning

#### Cross-Validation Results (5-Fold CV)

```python
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model, X_train, y_train, 
                            cv=10, scoring='r2')
```

| Model | Mean CV R¬≤ | Std Dev |
|-------|-----------|---------|
| Linear Regression | 0.5742 | 0.0312 |
| Decision Tree | 0.6892 | 0.0445 |
| Random Forest | **0.8123** | 0.0198 |
| Gradient Boosting | 0.7889 | 0.0267 |
| SVR | 0.6721 | 0.0389 |

#### Hyperparameter Tuning

**Random Forest:**
```python
from sklearn.model_selection import GridSearchCV

rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [20, 30, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(RandomForestRegressor(random_state=42), 
                          param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)
```





**Gradient Boosting:**
gb_params = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5]
}

**Decision Tree:**
dt_params = {
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

---

### 5Ô∏è‚É£ Selecting the Best Regression Model

## üèÜ Final Model Selection: **Gradient Boost Regressor**

### Justification

#### Quantitative Evidence
1. **Highest R¬≤ Score:** 0.68 (best variance explanation)
2. **Lowest RMSE:** 0.4900 (most accurate predictions)
3. **Lowest MAE:** 0.3156 (smallest typical error)
4. **Consistent CV Performance:** Low standard deviation (0.0198)

#### Qualitative Advantages
- ‚úÖ **Robust to outliers** in population and occupancy features
- ‚úÖ **Handles non-linearity** (captures geographic price patterns)
- ‚úÖ **No feature scaling required** (unlike SVR)
- ‚úÖ **Feature importance interpretability** (identifies key price drivers)
- ‚úÖ **Generalization ability** (ensemble reduces overfitting)




---

## üìà Results Summary

### Key Findings

1. Ensemble Methods Dominate:
   ‚Ä¢ Tree-based ensemble methods (Random Forest, Gradient Boosting)
     significantly outperform simple models
   
2. Feature Scaling Impact:
   ‚Ä¢ Critical for SVR and Linear Regression
   ‚Ä¢ Less important for tree-based methods
   
3. Hyperparameter Tuning Value:
   ‚Ä¢ Substantial performance improvements observed
   ‚Ä¢ Most critical for complex models (SVR, Gradient Boosting)
   
4. Model Complexity Trade-off:
   ‚Ä¢ Linear models too simple for this dataset
   ‚Ä¢ Single decision trees prone to overfitting
   ‚Ä¢ Ensemble methods strike the best balance
   
5. Dataset Characteristics:
   ‚Ä¢ Non-linear relationships between features and price
   ‚Ä¢ Feature interactions are important
   ‚Ä¢ Requires models that handle complexity well


---

## üöÄ Usage

### Running the Complete Pipeline

```python
# 1. Load and preprocess data
from src.preprocessing import load_and_preprocess_data
X_train, X_test, y_train, y_test = load_and_preprocess_data()

# 2. Train the best model
from src.models import train_best_model
best_model = train_best_model(X_train, y_train)

# 3. Make predictions
predictions = best_model.predict(X_test)

# 4. Evaluate
from src.evaluation import evaluate_model
metrics = evaluate_model(best_model, X_test, y_test)
print(metrics)
```



---

## üõ†Ô∏è Technologies Used

- **Python 3.8+**
- **scikit-learn** - Machine learning algorithms
- **pandas** - Data manipulation
- **numpy** - Numerical computations
- **matplotlib & seaborn** - Data visualization
- **jupyter** - Interactive notebooks

---

## üìä Project Insights

### What We Learned

1. **Ensemble methods** (Random Forest, Gradient Boosting) significantly outperform single models
2. **Geographic features** are crucial for housing price prediction
3. **Feature scaling** is essential for distance-based algorithms (SVR)
4. **Cross-validation** prevents overfitting and ensures model robustness
5. **Hyperparameter tuning** provides marginal but meaningful improvements

### Future Improvements

- [ ] Feature engineering (polynomial features, interactions)
- [ ] Advanced models (XGBoost, LightGBM, Neural Networks)
- [ ] Spatial analysis techniques for geographic data
- [ ] Time-series analysis if temporal data available
- [ ] Ensemble stacking of multiple models

---



---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Sreeraj A**
- Email: sreerajjdn89@gmail.com

---

## üôè Acknowledgments

- California Housing dataset provided by sklearn
- scikit-learn documentation and community
- [Your Institution/Course Name] for project guidance

---

## üìû Contact

For questions or suggestions, please open an issue or contact [your.email@example.com](sreerajjdn89.com)

---

**‚≠ê If you found this project helpful, please consider giving it a star!**# regression-california-dataset-
